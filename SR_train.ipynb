{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this Notebook, we train the Super Resoulation Module. \n",
    "- the training progress can be viewed W&B in tensorboard section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "from csbdeep.io import load_training_data\n",
    "from csbdeep.utils import axes_dict, plot_some,plot_history\n",
    "import matplotlib.pyplot as plt\n",
    "from model_DFCAN import DFCAN\n",
    "from loss_functions import mse_ssim, mse_ssim_psnr\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.models import load_model\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbEvalCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, mean_squared_error as mse, structural_similarity as ssim\n",
    "import numpy as np\n",
    "import wandb\n",
    "    \n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the train locations and logs.\n",
    "wandb.login()\n",
    "root_dir = '../F-actin'\n",
    "model_dir = Path(root_dir)/'SRModel'\n",
    "Path(model_dir).mkdir(exist_ok=True)\n",
    "train_data_file = f'{root_dir}/Train/SR/augmented_F-actin_02_SR_big.npz'\n",
    "log_dir = \"logs/fitSR/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "############## for saving the results of the training ################\n",
    "output_dir = Path.cwd() / 'SR_Model_plots_and_results'\n",
    "Path(output_dir).mkdir( exist_ok=True)\n",
    "\n",
    "# for saving the training progress and viewing it in tensorboard\n",
    "wandb.init(project=\"F-actin_SR\",name=f\"SR_train_MSE_SSIM_ep_2400_b_64\",config= tf.compat.v1.flags.FLAGS, sync_tensorboard=True)\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=wandb.run.dir)\n",
    "tf.function(jit_compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the training data and define the model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(['GPU:0'])\n",
    "\n",
    "with strategy.scope(): \n",
    "    ################  define the train  parameters  ################\n",
    "\n",
    "    init_lr = 1e-4\n",
    "    batch_size =64\n",
    "    epochs = 2400\n",
    "    beta_1=0.9\n",
    "    beta_2=0.999\n",
    "    scale_gt = 2.0\n",
    "    lr_decay_factor = 0.75\t# Learning rate decay factor\t\n",
    "\n",
    "    (X,Y), (X_val,Y_val), axes = load_training_data(train_data_file, validation_split=0.1, verbose=True)\n",
    "    print()\n",
    "    print()\n",
    "    print('Information about SR training data')\n",
    "    print(f\"X_shape :  {X.shape} ,\\nX_dtype : {X.dtype}   Y_shape: {Y.shape}\\nY_dtype : {Y.dtype}   ,\\nX_val : {X_val.shape} ,\\nY_val : {Y_val.shape}\")\n",
    "    print()\n",
    "\n",
    "    ############### preprocess the data to fit into the model ################\n",
    "    def preprocess_data(X, Y):\n",
    "        # Squeeze the unnecessary dimensions and transpose the axes\n",
    "        X = tf.squeeze(X, axis=-1)\n",
    "        Y = tf.squeeze(Y, axis=-1)\n",
    "        X = tf.transpose(X, perm=[0, 2, 3, 1])\n",
    "        Y = tf.transpose(Y, perm=[0, 2, 3, 1])\n",
    "        return X, Y\n",
    "\n",
    "    X, Y = preprocess_data(X, Y)\n",
    "    X_val, Y_val = preprocess_data(X_val, Y_val)\n",
    "\n",
    "    train_dataset = (X, Y)\n",
    "    val_dataset =(X_val, Y_val)\n",
    "\n",
    "\n",
    "    print(f'after preprocessing teh dta in batch chunks. X : {X.shape} Y : {Y.shape} X_val : {X_val.shape}  Y_val : {Y_val.shape}')\n",
    "\n",
    "    ################  plot some of the training data  ################\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plot_some(tf.transpose(X_val[:6], perm=[0, 3, 1, 2]),Y_val[:6])\n",
    "    plt.suptitle('5 example validation patches (top row: source, bottom row: target)')\n",
    "    plt.savefig(f'{output_dir}/SR_train_image_F-Actin_02_2400.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "    total_data,  height, width, channels= X.shape\n",
    "    print(f'total_data,  height, width, channels : {total_data,  height, width, channels}')\n",
    "    valid_data = val_dataset\n",
    "\n",
    "    Trainingmodel = DFCAN((height, width, channels), scale=scale_gt)\n",
    "    optimizer = Adam(learning_rate=init_lr, beta_1=beta_1, beta_2=beta_2)\n",
    "    Trainingmodel.compile(loss=mse_ssim, optimizer=optimizer)\n",
    "    #Trainingmodel.summary()\n",
    "    \n",
    "    tensorboard_callback = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    # lrate = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1) # monitor val_loss for faster training\n",
    "    \n",
    "    lrate= callbacks.ReduceLROnPlateau(monitor='val_loss', factor=lr_decay_factor, \n",
    "                            patience=15, mode='auto', min_delta=1e-4,\n",
    "                            cooldown=0, min_lr=init_lr*0.1, verbose=1)\n",
    "\n",
    "    hrate = callbacks.History()\n",
    "    \n",
    "    srate = callbacks.ModelCheckpoint(\n",
    "                str(model_dir),\n",
    "                monitor=\"loss\",\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,                \n",
    "                mode=\"auto\",\n",
    "            )\n",
    "\n",
    "    ################  load the model if it exists  ################\n",
    "    if len(os.listdir(model_dir)) > 0:\n",
    "    \n",
    "      with tf.keras.utils.custom_object_scope({'mse_ssim': mse_ssim}):\n",
    "        if len(os.listdir(model_dir)) > 0:\n",
    "            print(f'Loading model from {model_dir}')\n",
    "            Trainingmodel = load_model(model_dir)\n",
    "\n",
    "   \n",
    "    history = Trainingmodel.fit(X,Y, batch_size=batch_size,\n",
    "                                   epochs=epochs, validation_data=val_dataset, shuffle=True,\n",
    "                                   callbacks=[lrate, hrate, srate, tensorboard_callback ])\n",
    "    \n",
    "    \n",
    "    Trainingmodel.save(model_dir)\n",
    "        \n",
    "    \n",
    "    print(f'hisitry :: {history}')\n",
    "    print(sorted(list(history.history.keys())))\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plot_history(history,['loss','val_loss'])\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.savefig(f'{output_dir}/SR_train_image_F-Actin_02_2400_history.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_P = Trainingmodel.predict(X_val[:5])\n",
    "plot_some(tf.transpose(X_val[:5], perm=[0, 3, 1, 2]),Y_val[:5],_P,pmax=99.5)\n",
    "plt.suptitle('5 example validation patches\\n'      \n",
    "            'top row: input (source),  '          \n",
    "            'middle row: target (ground truth),  '\n",
    "            'bottom row: predicted from source')\n",
    "plt.savefig(f'{output_dir}/SR_train_image_F-Actin_02_2400_predictions.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc_thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
