{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from models import DFCAN\n",
    "from loss_functions import mse_ssim\n",
    "import tensorflow as tf\n",
    "from csbdeep.io import load_training_data\n",
    "from csbdeep.utils import plot_some\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "import tifffile\n",
    "from sim_fitting import cal_modamp, create_psf\n",
    "from csbdeep.utils import normalize\n",
    "import numpy.fft as F\n",
    "import mrcfile\n",
    "from pathlib import Path\n",
    "from read_otf import read_otf\n",
    "\n",
    "# Set up paths\n",
    "root_dir = '../Microtubules'\n",
    "dn_model_dir = Path(root_dir)/'DNModel'\n",
    "sr_model_dir = Path(root_dir)/'SRModel_1400_ready'\n",
    "# output_dir = Path(root_dir)/'DNModelOutput'\n",
    "output_dir = Path.cwd() / 'DN_Model_plots_and_results'\n",
    "Path(output_dir).mkdir( exist_ok=True)\n",
    "otf_path = 'TIRF488_cam1_0_z30_OTF2d.mrc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrc_file = f'/share/klab/argha/Microtubules/Test/Cell_019/RawSIMData_level_05.mrc'\n",
    "# first lets read teh file: \n",
    "with mrcfile.open(mrc_file, mode='r') as mrc:\n",
    "    full_image = mrc.data\n",
    "\n",
    "full_image = np.transpose(full_image, (1, 2, 0))\n",
    "print(f'full_image _ DN: {full_image.shape} , min: {np.min(full_image)} , max: {np.max(full_image)} , dtype : {full_image.dtype}' )\n",
    "\n",
    "def prctile_norm(x, min_prc=1, max_prc=99.9):\n",
    "    y = (x-np.percentile(x, min_prc))/(np.percentile(x, max_prc)-np.percentile(x, min_prc)+1e-7)\n",
    "    return y\n",
    "\n",
    "full_image = prctile_norm(full_image)\n",
    "print(f'full_image _ DN _ percentile normalized : {full_image.shape} , min: {np.min(full_image)} , max: {np.max(full_image)} , dtype : {full_image.dtype}' )\n",
    "\n",
    "plt.imshow(full_image[:,:,0])\n",
    "plt.title(f'full_image_ DN : shape: {full_image.shape}, \\n min: {np.min(full_image)},\\n max: {np.max(full_image)},\\n dtype: {full_image.dtype}')\n",
    "plt.axis('off')\n",
    "plt.savefig(f'{output_dir}/DN_TEST_full_image_input.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "height, width, channels = 128, 128, 9\n",
    "print(f'height: {height} , width: {width} , channels: {channels}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   Pparameter Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "init_lr = 1e-4\n",
    "lr_decay_factor = 0.5\t# Learning rate decay factor\t\n",
    "\n",
    "batch_size = 6\n",
    "epochs = 1\n",
    "beta_1=0.9\n",
    "beta_2=0.999\n",
    "wavelength = 0.488 \n",
    "excNA = 1.35\n",
    "dx = 62.6e-3\n",
    "dy = dx\n",
    "dxy = dx \n",
    "scale = 2.0\n",
    "setupNUM = 1\n",
    "space = wavelength/excNA/2\n",
    "k0mod = 1 / space\n",
    "napodize = 10\n",
    "nphases = 3\n",
    "ndirs = 3\n",
    "sigma_x = 0.5\n",
    "sigma_y = 0.5\n",
    "recalcarrays = 2\n",
    "ifshowmodamp = 0\n",
    "norders = int((nphases + 1) / 2)\n",
    "phase_space = 2 * np.pi / nphases\n",
    "\n",
    "Nx = height\n",
    "Ny = width\n",
    "Nx_hr= Nx * scale\n",
    "Ny_hr =  Ny * scale\n",
    "\n",
    "[Nx_hr, Ny_hr] = [Nx* scale, Ny* scale] \n",
    "[dx_hr, dy_hr] = [x / scale for x in [dxy, dxy]]\n",
    "\n",
    "xx = dx_hr * np.arange(-Nx_hr / 2, Nx_hr / 2, 1)\n",
    "yy = dy_hr * np.arange(-Ny_hr / 2, Ny_hr / 2, 1)\n",
    "[X, Y] = np.meshgrid(xx, yy)\n",
    "\n",
    "dkx = 1.0 / ( Nx *  dxy)\n",
    "dky = 1.0 / ( Ny * dxy)\n",
    "dkr = np.min([dkx, dky])\n",
    "\n",
    "\n",
    "\n",
    "setupNUM == 0\n",
    "\n",
    "if setupNUM == 0:\n",
    "     k0angle_c = [1.48, 2.5272, 3.5744]\n",
    "     k0angle_g = [0.0908, -0.9564, -2.0036]  \n",
    "if setupNUM == 1:\n",
    "     k0angle_c = [-1.66, -0.6128, 0.4344]\n",
    "     k0angle_g = [3.2269, 2.1797, 1.1325]      \n",
    "if setupNUM == 2:\n",
    "     k0angle_c = [1.5708, 2.618, 3.6652]\n",
    "     k0angle_g = [0, -1.0472, -2.0944] \n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'Ny': height,\n",
    "    'Nx': width,\n",
    "    'wavelength':wavelength,\n",
    "    'excNA':excNA,\n",
    "    'ndirs':ndirs,\n",
    "    'nphases':nphases,\n",
    "    'init_lr': init_lr,\n",
    "    'ifshowmodamp':ifshowmodamp,\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'beta_1':beta_1,\n",
    "    'beta_2':beta_2,\n",
    "    'scale_gt': scale,\n",
    "    'setupNUM': setupNUM,\n",
    "    'k0angle_c':k0angle_c,\n",
    "    'k0angle_g':k0angle_g,\n",
    "    'recalcarrays':recalcarrays,\n",
    "    'dxy':dxy,\n",
    "    'space':space,\n",
    "    'k0mod':k0mod,\n",
    "    'norders':norders,\n",
    "    'napodize':napodize,\n",
    "    'scale': scale,\n",
    "    'sigma_x': sigma_x,\n",
    "    'sigma_y': sigma_y,\n",
    "    #'log_dir': log_dir,\n",
    "    'den_model_dir': dn_model_dir,\n",
    "    'sr_model_dir': sr_model_dir\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'typing_extensions'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the SR model\n",
    "sr_model_path = sr_model_dir\n",
    "if len(os.listdir(sr_model_path)) > 0:\n",
    "    print(f\"Loading model from {sr_model_path}\")\n",
    "    with tf.keras.utils.custom_object_scope({'mse_ssim': mse_ssim}):\n",
    "        sr_trained_model = load_model(sr_model_path)\n",
    "\n",
    "# Load the DN model\n",
    "dn_model_path = dn_model_dir\n",
    "if len(os.listdir(dn_model_path)) > 0:\n",
    "    print(f\"Loading model from {dn_model_path}\")\n",
    "    with tf.keras.utils.custom_object_scope({'mse_ssim': mse_ssim}):\n",
    "        dn_trained_model = load_model(dn_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step:1 : CHUNK THE IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_image(image, chunk_size):\n",
    "\n",
    "    chunks = []\n",
    "    chunk_coords = []\n",
    "    image_height, image_width = image.shape[:2]\n",
    "\n",
    "    # Iterate over the image with steps of chunk_size\n",
    "    for y in range(0, image_height, chunk_size):\n",
    "        for x in range(0, image_width, chunk_size):\n",
    "            #print(y , x)\n",
    "            # Calculate end coordinates\n",
    "            y_end = min(y + chunk_size, image_height)\n",
    "            x_end = min(x + chunk_size, image_width)\n",
    "            #print(y_end, x_end)\n",
    "            if y== 384:\n",
    "              y = 502-128\n",
    "            if x == 384:\n",
    "              x = 502-128\n",
    "\n",
    "\n",
    "            # Extract chunk\n",
    "            chunk = image[y:y_end, x:x_end]\n",
    "            # chunk = prctile_norm(chunk)\n",
    "            #print(chunk.shape)\n",
    "            chunks.append(chunk)\n",
    "            chunk_coords.append((x, y))\n",
    "\n",
    "    return chunks, chunk_coords\n",
    "\n",
    "resized_image = full_image\n",
    "\n",
    "chunk_size = 128\n",
    "chunks, chunk_coords = chunk_image(resized_image, chunk_size)\n",
    "print(f'after chunkinh: {len(chunks)} :: {len(chunk_coords)} :: {type(chunks)} {type(chunk_coords)}')\n",
    "chunks= np.array(chunks).astype(np.float32)\n",
    "\n",
    "print(f'chunks: {chunks.shape} :: {chunks.dtype} :: {type(chunks)}')\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the chunks in a grid layout\n",
    "num_chunks = chunks.shape[0]\n",
    "ncols = int(np.ceil(np.sqrt(num_chunks)))\n",
    "nrows = int(np.ceil(num_chunks / ncols))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.suptitle('Chunks')\n",
    "print(f'chunks: {chunks.shape} {type(chunks)} : chunk_coords: {len(chunk_coords)}{type(chunk_coords)} ')\n",
    "for i, (chunk, (x_start, y_start)) in enumerate(zip(chunks, chunk_coords)):\n",
    "    print(f'chunk.shape: {chunk.shape} :: {type(chunk)} , {x_start}, {y_start}')\n",
    "    print(f'min value: {np.min(chunk)} :: max value: {np.max(chunk)} dtype: {chunk.dtype}')\n",
    "    plt.subplot(nrows, ncols, i + 1)\n",
    "    plt.imshow(chunk[...,1])\n",
    "    plt.title(f\"({x_start}, {y_start}):S {chunk.shape}\", fontsize=8)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{output_dir}/DN_TEST_chunk_.png')\n",
    "    # plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2:  DO THE DN PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upscale_chunks(chunks, chunk_coords, upscale_factor=2):\n",
    "\n",
    "\n",
    "    upscaled_chunks = []\n",
    "    upscaled_chunk_coords = []\n",
    "    # will be feed into prediction from SR\n",
    "    chunks = np.array(chunks)\n",
    "    print(chunks.shape)\n",
    "    predictions = sr_trained_model.predict(chunks)\n",
    "\n",
    "    OTF, prol_OTF, PSF = read_otf(otf_path, Nx_hr, Ny_hr, dkx, dky, dkr)\n",
    "     \n",
    "    print(f'PSF: {PSF.shape} {PSF.dtype}:: OTF: {OTF.shape} {OTF.dtype}')     \n",
    "\n",
    "    def psf_otf ():\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 15))\n",
    "        axes[0].imshow(PSF)\n",
    "        axes[0].set_title('PSF')\n",
    "        \n",
    "        axes[1].imshow(abs(OTF))\n",
    "        axes[1].set_title('OTF')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/PSF_OTF_Prediction.png', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    psf_otf ()\n",
    "\n",
    "    def _get_cur_k( image_gt):\n",
    "\n",
    "        print(f'inside _get_cur_k: {image_gt.shape}')\n",
    "        \n",
    "        cur_k0, modamp = cal_modamp(np.array(image_gt).astype(np.float32), prol_OTF, parameters)\n",
    "        cur_k0_angle = np.array(np.arctan2(cur_k0[:, 1] , cur_k0[:, 0]))\n",
    "        cur_k0_angle[1:parameters['ndirs']] = cur_k0_angle[1:parameters['ndirs']] + np.pi\n",
    "        cur_k0_angle = -(cur_k0_angle - np.pi/2)\n",
    "        for nd in range(parameters['ndirs']):\n",
    "            if np.abs(cur_k0_angle[nd] - parameters['k0angle_g'][nd]) > 0.05:\n",
    "                cur_k0_angle[nd] = parameters['k0angle_g'][nd]\n",
    "        cur_k0 = np.sqrt(np.sum(np.square(cur_k0), 1))\n",
    "        given_k0 = 1 / parameters['space']\n",
    "        cur_k0[np.abs(cur_k0 - given_k0) > 0.1] = given_k0\n",
    "            \n",
    "        \n",
    "        return cur_k0, cur_k0_angle, modamp\n",
    "\n",
    "\n",
    "    def reshape_to_3_channels( batch):\n",
    "        \n",
    "        B, H, W, C = batch.shape\n",
    "        assert C % ndirs == 0, \"The last dimension must be divisible by 3\"\n",
    "        new_batch_size = B * (C // ndirs)\n",
    "        return batch.reshape(new_batch_size, H, W, nphases)\n",
    " \n",
    "\n",
    "    def _phase_computation( img_SR, modamp, cur_k0_angle, cur_k0):\n",
    "           \n",
    "\n",
    "            phase_list = -np.angle(modamp)\n",
    "            print(f'phase_list: {phase_list},  {len(phase_list)}')\n",
    "            img_gen = []\n",
    "            for d in range(ndirs):\n",
    "                alpha = cur_k0_angle[d]\n",
    "                \n",
    "                for i in range(nphases):\n",
    "                    kxL = cur_k0[d] * np.pi * np.cos(alpha)\n",
    "                    kyL = cur_k0[d] * np.pi * np.sin(alpha)\n",
    "                    kxR = -cur_k0[d] * np.pi * np.cos(alpha)\n",
    "                    kyR = -cur_k0[d] * np.pi * np.sin(alpha)\n",
    "                    phOffset = phase_list[d] + i * phase_space\n",
    "                    interBeam = np.exp(1j * (kxL * X + kyL * Y + phOffset)) + np.exp(1j * (kxR * X + kyR * Y))\n",
    "                    pattern = normalize(np.square(np.abs(interBeam)))\n",
    "                    patterned_img_fft = F.fftshift(F.fft2(pattern * img_SR)) * OTF\n",
    "                    modulated_img = np.abs(F.ifft2(F.ifftshift(patterned_img_fft)))\n",
    "                    modulated_img = normalize(cv2.resize(modulated_img, (Ny, Nx)))    \n",
    "                    img_gen.append(modulated_img)\n",
    "\n",
    "                    ####################    Plotting the patterns modulated image    ############################\n",
    "\n",
    "                    # print(f'pattern shape: {pattern.shape} min {pattern.min()} max {pattern.max()} dtype {pattern.dtype}')\n",
    "                    \n",
    "                    # if d in [0, 1, 2]:\n",
    "                    #     plt.figure(figsize=(25, 12))  # Adjust size as needed\n",
    "                    #     plt.subplot(1, 5, 1)\n",
    "                    #     plt.imshow(img_SR)\n",
    "                    #     plt.title(f'img_SR shape: {img_SR.shape} \\n min {np.min(img_SR)} \\n max {np.max(img_SR)} \\n dtype {img_SR.dtype}')\n",
    "                    #     plt.axis('off')\n",
    "\n",
    "                    #     plt.subplot(1, 5, 2)\n",
    "                    #     plt.imshow(pattern,)\n",
    "                    #     plt.title(f'pattern_{d} shape: {pattern.shape} \\n min {pattern.min()}\\n max {pattern.max()} \\n dtype {pattern.dtype}')\n",
    "                    #     plt.axis('off')\n",
    "\n",
    "                    # # patterned_img_fft = F.fftshift(F.fft2(pattern * img_SR)) * OTF\n",
    "\n",
    "                    # # print(f'patterned_img_fft shape: {patterned_img_fft.shape} min {patterned_img_fft.min()} max {patterned_img_fft.max()} dtype {patterned_img_fft.dtype}')\n",
    "                    # # patterned_img_fft shape: (128, 128) min (-75.88900177706913-118.26764698032483j) max (1058.9433023205993+0j) dtype complex128\n",
    "                    # if d in [0, 1, 2]:\n",
    "                    #     plt.subplot(1, 5, 3)\n",
    "                    #     # Compute magnitude\n",
    "                    #     magnitude = np.abs(patterned_img_fft)\n",
    "\n",
    "                    #     plt.imshow(magnitude , cmap = 'gray')  # Use 'gray' colormap for grayscale\n",
    "                    #     plt.title(f' patterned_img_fft_{d} : Magnitude shape: {magnitude.shape}\\nmin: {magnitude.min()}\\nmax: {magnitude.max()}')\n",
    "                    #     plt.axis('off')  # Hide axis ticks and labels\n",
    "                    #     # plt.colorbar()  # Optionally add a color bar to indicate scale\n",
    "                    #     # plt.show()\n",
    "                \n",
    "                \n",
    "                    #     plt.subplot(1, 5, 4)\n",
    "                    #     phase = np.angle(patterned_img_fft)\n",
    "                    #     plt.imshow(phase, cmap='gray' )  # 'hsv' colormap shows phase as color\n",
    "                    #     plt.title(f'patterned_img_fft_{d} : Phase shape: {phase.shape}\\nmin: {np.min(phase)}\\nmax: {np.max(phase)}')\n",
    "                    #     plt.axis('off')  # Hide axis ticks and labels\n",
    "                    #     print(f'modulated_img shape: {modulated_img.shape} min {modulated_img.min()} max {modulated_img.max()} dtype {modulated_img.dtype}')\n",
    "                    #     plt.subplot(1, 5, 5)\n",
    "                    #     plt.imshow(modulated_img)\n",
    "                        \n",
    "                    #     plt.title(f'modulated_img_{d} shape: {modulated_img.shape} \\nmin {modulated_img.min()} \\nmax {modulated_img.max()} \\ndtype {modulated_img.dtype}')\n",
    "\n",
    "\n",
    "                    #     plt.axis('off')\n",
    "                    #     plt.tight_layout()  # Adjust the subplots to fit into the figure\n",
    "\n",
    "                    #     # Save and show the figure for the current phase\n",
    "                    #     print('\\n\\n\\n\\n\\n\\n\\n')\n",
    "                    #     print('all plots have been saved')\n",
    "                    #     plt.savefig(f'{output_dir}/DN_Test_phase_{d}_patterns.png')  # Save the figure\n",
    "                    \n",
    "                    \n",
    "\n",
    "            \n",
    "            \n",
    "            img_gen = np.asarray(img_gen)\n",
    "      \n",
    "            \n",
    "            return img_gen\n",
    "    ##############################################################################################################\n",
    "\n",
    "\n",
    "    input_PFE_batch = []\n",
    "    input_MPE_batch = []\n",
    "    list_image_in = []\n",
    "    list_image_gen = []\n",
    "    print(f'this is goign for prediction: {chunks.shape}')\n",
    "    sr_predictions = sr_trained_model.predict(chunks)\n",
    "    print(f'sr prediction: {sr_predictions.shape} : min_value : {np.min(sr_predictions)} :: max_value : {np.max(sr_predictions)} : dtype : {sr_predictions.dtype}') # sr prediction: (15, 256, 256)\n",
    "    sr_predictions = tf.squeeze(sr_predictions, axis=-1)\n",
    "    print(f'sr prediction: {sr_predictions.shape}') # sr prediction: (15, 256, 256)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(sr_predictions.shape[0]):\n",
    "        image_in = chunks[i:i+1][0]  \n",
    "        print(f' from for loop  image_in: {image_in.shape}')\n",
    "        list_image_in.append(image_in)\n",
    "\n",
    "        # for image gen\n",
    "        img_SR = sr_predictions[i:i+1][0]\n",
    "        print(f'from for loop image_SR: {img_SR.shape}')\n",
    "        # cur_k0, cur_k0_angle, modamp = self._get_cur_k(image_gt=image_in)\n",
    "        # cur_k0, modamp = cal_modamp(np.array(image_in).astype(np.float32), OTF, parameters)\n",
    "        cur_k0, cur_k0_angle, modamp = _get_cur_k(image_gt=image_in) # here is confussion ????\n",
    "\n",
    "        image_gen = _phase_computation(img_SR, modamp, cur_k0_angle, cur_k0)\n",
    "   \n",
    "        print(f'image_gen: {image_gen.shape}  from phase compution')\n",
    "        image_gen = np.transpose(image_gen, (1, 2, 0))\n",
    "        list_image_gen.append(image_gen)\n",
    "\n",
    "    input_PFE_batch = np.asarray(list_image_in)\n",
    "    input_PFE_batch = reshape_to_3_channels(input_PFE_batch)\n",
    "\n",
    "    input_MPE_batch = np.asarray(list_image_gen)\n",
    "    input_MPE_batch = reshape_to_3_channels(input_MPE_batch)\n",
    "    ## plotting MPE BRANCH : moire pattern extracting: happens in DN\n",
    "    num_images = input_MPE_batch.shape[0]  # Number of images in the batch\n",
    "    plt.figure(figsize=(15, 5))  # Create a figure to hold the images, adjust figsize as needed\n",
    "    plt.title(f'MPE batch : shape : {input_MPE_batch.shape}  ')\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)  # Create subplots for each image\n",
    "        plt.imshow(input_MPE_batch[i])  # Display the i-th image\n",
    "        plt.axis('off')  # Turn off axis labels and ticks\n",
    "        plt.suptitle(f'MPE : {input_PFE_batch[i].shape}')\n",
    "    \n",
    "    plt.savefig(f'{output_dir}/DN_Test_input_MPE_batch.png')  # Save the figure\n",
    "    plt.show()\n",
    " \n",
    "\n",
    "\n",
    "    ## plotting PFE Branch: pattern formation: happens in SR\n",
    "    num_images = input_PFE_batch.shape[0]  # Number of images in the batch\n",
    "    plt.figure(figsize=(15, 5))  # Create a figure to hold the images, adjust figsize as needed\n",
    "    plt.title(f'PFE batch : shape : {input_PFE_batch.shape}  ')\n",
    "\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(input_PFE_batch[i])\n",
    "        plt.axis('off')\n",
    "       \n",
    "        plt.suptitle(f'PFE : {input_PFE_batch[i].shape}')\n",
    "    plt.savefig(f'{output_dir}/DN_Test_input_PFE_batch.png')\n",
    "    plt.show()\n",
    "\n",
    "    # input MPE (90, 128, 128, 3), input PFE (90, 128, 128, 3),gt (90, 128, 128, 3)\n",
    "    print(f'inptu_MPE_batch: {input_MPE_batch.shape} :: input_PFE_batch: {input_PFE_batch.shape}')\n",
    "\n",
    "    def reshape_to_9_channels( batch):\n",
    "        \n",
    "        B, H, W, C = batch.shape\n",
    "        print(f'B: {B} , H: {H}, W: {W} , C: {C} ')\n",
    "        \n",
    "        new_batch_size = int(B / (ndirs * nphases / C))\n",
    "        print(f' new_batch_size : { new_batch_size} B: {B} , H: {H}, W: {W} , C: {C} ')\n",
    "        return batch.reshape(new_batch_size, H, W, ndirs * nphases)\n",
    "\n",
    "\n",
    "\n",
    "    predictions = dn_trained_model.predict([input_MPE_batch, input_PFE_batch]) # prediction : (45, 128, 128, 3)\n",
    "    #predictions = np.squeeze(predictions)\n",
    "    print(f'prediction : {predictions.shape}') # prediction : (45, 128, 128, 3) --Y 15, 3 , 128, 128, 3\n",
    "    predictions = reshape_to_9_channels(predictions)\n",
    "    print(f'prediction reshape to channel 9 : {predictions.shape}\\n : min_value : {np.min(predictions)}\\n :: max_value : {np.max(predictions)}\\n : dtype : {predictions.dtype}') # prediction : (45, 128, 128, 3) --Y 15, 3 , 128, 128, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RE-ASSEMBLE THE IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 1004\n",
    "channels = 9  # Number of channels in the image\n",
    "\n",
    "# Initialize the final image with zeros\n",
    "final_image = np.zeros((target_size, target_size, channels))\n",
    "\n",
    "for i, (upscaled_chunk, (x_start, y_start)) in enumerate(zip(upscaled_chunks, upscaled_chunk_coords)):\n",
    "    # # Ensure the chunk has a third dimension (channels)\n",
    "    # if len(upscaled_chunk.shape) == 2:  # Shape is (256, 256)\n",
    "    #     upscaled_chunk = np.expand_dims(upscaled_chunk, axis=-1)  # Shape becomes (256, 256, 1)\n",
    "\n",
    "    # # Now, safely check the number of channels\n",
    "    # if upscaled_chunk.shape[2] != channels:\n",
    "    #     raise ValueError(f\"Chunk has {upscaled_chunk.shape[2]} channels, expected {channels} channels.\")\n",
    "    print('inside the  reassemble loop')\n",
    "    print(f'upscaled_chunk: {upscaled_chunk.shape}  :: {type(upscaled_chunk)} dtype: {upscaled_chunk.dtype} :: min_value: {np.min(upscaled_chunk)} :: max_value: {np.max(upscaled_chunk)}')\n",
    "    \n",
    "    x_end = min(x_start + upscaled_chunk.shape[1], target_size)\n",
    "    y_end = min(y_start + upscaled_chunk.shape[0], target_size)\n",
    "\n",
    "    # Ensure that x_end and y_end are valid indices\n",
    "    if x_end > x_start and y_end > y_start:\n",
    "        final_image[y_start:y_end, x_start:x_end, :] = np.maximum(\n",
    "            final_image[y_start:y_end, x_start:x_end, :],\n",
    "            upscaled_chunk[:y_end-y_start, :x_end-x_start, :]\n",
    "        )\n",
    "\n",
    "tifffile.imwrite(f'{output_dir}/prediction _ fullimage.tif', final_image.transpose(1,2,0))\n",
    "# Visualize the final image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(f'Reassembled Image _DN  :: shape :: {final_image.shape} \\n final_imahge:dtype :: {final_image.dtype} :: \\n min_value: {np.min(final_image)} ::\\n max_value: {np.max(final_image)}')\n",
    "plt.imshow(final_image[...,1])\n",
    "\n",
    "plt.axis('off')\n",
    "plt.savefig(f'{output_dir}/DN_Test_final_image.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
